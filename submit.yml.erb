<%
  groups = OodSupport::User.new.groups.map(&:name).drop(1)

  # your image location will differ
  ppn = num_cores.blank? ? 1 : num_cores.to_i
  walltime = (bc_num_hours.to_i * 60)
%>
---
batch_connect:
  template: basic
  websockify_cmd: '/usr/bin/websockify'
  script_wrapper: |

    module purge
    module load singularity
    <%- if virtual_env != "" %>
    module load python-miniconda3/4.12.0
    source activate <%= virtual_env %>
    <%- else %>
    # This is to ensure that all "other modules" can be loaded (even
    # those that require R be loaded first) while also loading R
    # last
    module load git/2.37.2
    module load <%= version %>
    module load <%= other_modules %>
    module load <%= version %>
    module load openssl/1.0.2k-gcc-4.8.5
    <%- end %>
    cat << "CTRSCRIPT" > container.sh
    export PATH="$PATH:/opt/TurboVNC/bin"
    %s  
    CTRSCRIPT

    # What is the OS *of the host*
    HOST_OS=`uname -r`
    # Where is this container
    if [[ $HOST_OS =~ "el8" ]]; then
        export container_image=/software/rhel8/quest_ondemand/quest_ood_rstudio_server/rhel8-rstudio-server-2024.09.0-394-chrome-114.0.5735.90.sif
    elif [[ $HOST_OS =~ "el7" ]]; then
        export container_image=/software/openondemand/rstudio-server/rserver-ood.sif
    else
        echo "Host OS not recognized, no container found"
    fi

    # Set working directory to home directory

    export SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT=0
    R_HOME=$(R RHOME)
    R_VERSION=$(R --version | head -1 | awk '{print $3}')
    R_EXEC=`which R --skip-alias`

    # Need a unique /tmp for this job for /tmp/rstudio-rsession & /tmp/rstudio-server
    WORKDIR=${HOME}/rserver/${SLURM_JOB_ID}
    mkdir -m 700 -p ${WORKDIR}/tmp/rstudio-server  
    mkdir -p ${WORKDIR}/var/run/{lock/rstudio-server,rstudio-server/rstudio-rsession,mount,systemd} ${WORKDIR}/var/lib/rstudio-server ${WORKDIR}/logs/{rstudio,rstudio-server}
    uuidgen > ${WORKDIR}/tmp/rstudio-server/secure-cookie-key
    chmod 600 ${WORKDIR}/tmp/rstudio-server/secure-cookie-key

    cat >${WORKDIR}/rserver.conf <<END
    # Server Configuration File
    rsession-which-r=${R_EXEC}
    auth-timeout-minutes=0
    rsession-proxy-max-wait-secs=45
    auth-stay-signed-in-days=10
    session-use-file-storage=1
    <%- if virtual_env != "" %>
    rsession-ld-library-path=${CONDA_PREFIX}/lib
    <%- else %>
    rsession-ld-library-path=${LD_LIBRARY_PATH}
    <%- end %>
    END

    cat > ${WORKDIR}/rsession.conf <<END
    session-timeout-minutes=0
    session-save-action-default=no
    END

    # rsession needs to start with OMP_NUM_THREADS set to prevent OpenBLAS
    # (and any other OpenMP-enhanced libraries used by R) from spawning too many
    # threads. a wrapper script is created for rsession.
    #
    cat > ${WORKDIR}/rsession.sh <<END
    #!/bin/sh
    set -o xtrace
    R_DOC_DIR=$R_HOME/doc
    R_INCLUDE_DIR=$R_HOME/include
    R_SHARE_DIR=$R_HOME/share
    RSTUDIO_DEFAULT_R_VERSION_HOME=$R_HOME
    RSTUDIO_DEFAULT_R_VERSION=$R_VERSION
    PATH=${PATH}:/usr/lib/rstudio-server/bin
    export PKG_CONFIG_PATH=${PKG_CONFIG_PATH}
    export OMP_NUM_THREADS=${SLURM_NPROCS}
    export MC_CORES=$((${SLURM_NPROCS}-1))
    export SLURM_NPROCS=${SLURM_NPROCS}
    export SLURM_NTASKS=${SLURM_NTASKS}
    export INCLUDE=${INCLUDE}
    export C_INCLUDE_PATH=${C_INCLUDE_PATH}
    export CPLUS_INCLUDE_PATH=${CPLUS_INCLUDE_PATH}
    /usr/lib/rstudio-server/bin/rsession "\${@}" 
    END

    chmod +x ${WORKDIR}/rsession.sh

    # [Rstudio 1.4] database is required
    cat > ${WORKDIR}/database.conf <<END
    provider=sqlite
    directory=/var/lib/rstudio-server
    END

    # [Rstudio 1.4] configure logging
    cat > ${WORKDIR}/logging.conf <<END
    [*]
    log-level=debug
    max-size-mb=10
    END

    cat > ${WORKDIR}/chrome-webgl <<END
    Xvfb -ac :4070 -screen 0 1280x1024x16 &
    export DISPLAY=:4070
    LD_LIBRARY_PATH="" /opt/chrome/linux-114.0.5735.90/chrome-linux64/chrome --bwsi --disable-metrics --no-first-run --disable-translate --no-sandbox --headless --enable-webgl --disable-gpu "\${@}"
    END

    cat > ${WORKDIR}/chrome <<END
    LD_LIBRARY_PATH="" /opt/chrome/linux-114.0.5735.90/chrome-linux64/chrome --bwsi --disable-metrics --no-first-run --disable-translate --no-sandbox --headless "\${@}"
    END

    chmod +x ${WORKDIR}/chrome
    chmod +x ${WORKDIR}/chrome-webgl

    ## some binds that are necessary for running with singularity
    export SING_BINDS=" --bind ${WORKDIR}/logs/rstudio-server:/var/log/rstudio-server --bind ${WORKDIR}/logs/rstudio:/var/log/rstudio --bind ${WORKDIR}/var/run:/var/run --bind ${WORKDIR}/var/lib/rstudio-server:/var/lib/rstudio-server  --bind ${WORKDIR}/tmp:/tmp"

    ## use our specific configs
    export SING_BINDS="$SING_BINDS --bind ${WORKDIR}/rsession.conf:/etc/rstudio/rsession.conf --bind ${WORKDIR}/rserver.conf:/etc/rstudio/rserver.conf --bind ${WORKDIR}/database.conf:/etc/rstudio/database.conf --bind ${WORKDIR}/rsession.sh:/etc/rstudio/rsession.sh --bind ${WORKDIR}/logging.conf:/etc/rstudio/logging.conf --bind ${WORKDIR}/chrome:/etc/rstudio/chrome"

    ## quest stuff
    export SING_BINDS="$SING_BINDS --bind /software:/software --bind /hpc/software:/hpc/software"

    # Only bind projects directories that the user would have access to based on their unix groups
    <%- groups.each do |group| %>
    export SING_BINDS="$SING_BINDS <%= File.directory?("/projects/#{group}") ? "--bind /projects/#{group}:/projects/#{group}" : "" %>"
    <%- end %>

    # only bind /kellogg is individual is part of kellogg group
    export SING_BINDS="$SING_BINDS <%= groups.include?('kellogg') ? "--bind /kellogg/:/kellogg/" : "" %>"

    # only bind /scratch/<netid> if individual has a scratch space
    export SING_BINDS="$SING_BINDS <%= File.directory?("/scratch/#{User.new.name}") ? "--bind /scratch/#{User.new.name}:/scratch/#{User.new.name}" : "" %>"

    export SINGULARITYENV_PATH="$PATH"

    ## Need to add the --nv flag if we are running on a GPU
    <%- if gres_value != "" %>
    export SING_GPU="--nv"
    <%- else %>
    export SING_GPU=""
    <%- end %>

    <%- if start_clean_session == "1" %>
    export SINGULARITYENV_XDG_DATA_HOME=${WORKDIR}
    <%- end %>

    singularity exec $SING_GPU $SING_BINDS "${container_image}" /bin/bash container.sh

  header: | 
    #!/bin/bash
        . ~/.bashrc

script:
   <%- if user_email != "" %>
   email_on_started: true
   <%- end %>
   native:
     # What partition is the user submitting to
     - "--partition"
     - "<%= slurm_partition %>"
     # Under what account is the user submitting this job
     - "--account"
     - "<%= slurm_account %>"
     # How much time (in hours)
     - "--time"
     - "<%= walltime %>"
     # How many nodes (always 1)
     <%- if number_of_nodes != "" %>
     - "--nodes"
     - "<%= number_of_nodes %>"
     <%- else %>
     - "--nodes"
     - "1"
     <%- end %>
     # How many CPUs
     - "--ntasks-per-node"
     - "<%= ppn %>"
     # How much memory
     - "--mem"
     - "<%= memory_per_node %>G"
     # Job Name
     - "--job-name"
     - "<%= job_name %>"
     # If the user supplies an e-mail, then they will get an e-mail when the job begins
     <%- if user_email != "" %>
     - "--mail-user"
     - "<%= user_email %>"
     <%- end %>
     # If the user requested a GPU, then we need to add this argument to our job submit command
     <%- if gres_value != "" %>
     - "--gres"
     - "<%= gres_value %>"
     <%- end %>
     # If the user requested some kind of constraint, the apply that
     <%- if constraint != "" %>
     - "--constraint"
     - "<%= constraint %>"
     <%- end %>
